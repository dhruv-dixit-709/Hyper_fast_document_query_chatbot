{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d7990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo_run as run\n",
    "from nemo.collections import llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a904921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path(s) if different:\n",
    "ROOT_DIR = \"/workspace\"\n",
    "MODEL_PATH = f\"{ROOT_DIR}/Llama-3.1-8B-nemo\"\n",
    "SEQ_LENGTH = 8192\n",
    "EXP_NAME = \"Llama-3.1-8B-nemo-ft\"\n",
    "EXP_DIR = f\"{ROOT_DIR}/{EXP_NAME}\"\n",
    "DATA_PATH = f\"{ROOT_DIR}/SAT_DATA-data\"\n",
    "DATA_PATHS = {\n",
    "    \"train\": [1.0, f\"{DATA_PATH}/SAT_DATA_tokenized_train_text_document\"],\n",
    "    \"validation\": [f\"{DATA_PATH}/SAT_DATA_tokenized_test_text_document\"],\n",
    "    \"test\": [f\"{DATA_PATH}/SAT_DATA_tokenized_val_text_document\"],\n",
    "}\n",
    "INDEX_MAPPING_DIR = f\"{DATA_PATH}/index_mappings\"\n",
    "\n",
    "# Change these to accommodate resources:\n",
    "DEVICES = 8\n",
    "NODES = 1\n",
    "TENSOR_PARALLEL_SIZE = DEVICES\n",
    "PIPELINE_PARALLEL_SIZE = NODES\n",
    "MICRO_BATCH_SIZE = 4\n",
    "\n",
    "# Change the fine-tuning recipe for your model and dataset (below values for demonstration purposes):\n",
    "STEPS = 30\n",
    "GLOBAL_BATCH_SIZE = 128\n",
    "LR = 1e-4\n",
    "MIN_LR = 1e-5\n",
    "WARMUP_STEPS = 2\n",
    "LOG_INTERVAL = 1\n",
    "VAL_INTERVAL = 10\n",
    "NUM_VAL_BATCHES = 5\n",
    "\n",
    "\n",
    "def configure_recipe():\n",
    "    # Define the recipe\n",
    "    recipe = llm.llama31_8b.finetune_recipe(\n",
    "        num_nodes=NODES,\n",
    "        num_gpus_per_node=DEVICES,\n",
    "        peft_scheme=None,  # Full finetuning\n",
    "        seq_length=SEQ_LENGTH,\n",
    "    )\n",
    "    recipe.resume.restore_config.path = MODEL_PATH\n",
    "    recipe.log.explicit_log_dir = EXP_DIR\n",
    "    recipe.log.ckpt.every_n_train_steps = VAL_INTERVAL\n",
    "\n",
    "    # Change dataset (default is Squad dataset)\n",
    "    recipe.data = run.Config(\n",
    "        llm.PreTrainingDataModule,\n",
    "        paths=DATA_PATHS,\n",
    "        index_mapping_dir=INDEX_MAPPING_DIR,\n",
    "        seq_length=SEQ_LENGTH,\n",
    "        micro_batch_size=MICRO_BATCH_SIZE,\n",
    "        global_batch_size=GLOBAL_BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    # Set the training parameters if you dont want to use the recipe defaults\n",
    "    recipe.trainer.max_steps = STEPS\n",
    "    recipe.trainer.log_every_n_steps = LOG_INTERVAL\n",
    "    recipe.trainer.val_check_interval = VAL_INTERVAL\n",
    "    recipe.trainer.limit_val_batches = NUM_VAL_BATCHES\n",
    "    recipe.trainer.strategy.tensor_model_parallel_size = TENSOR_PARALLEL_SIZE\n",
    "    recipe.trainer.strategy.pipeline_model_parallel_size = PIPELINE_PARALLEL_SIZE\n",
    "    recipe.trainer.strategy.sequence_parallel = TENSOR_PARALLEL_SIZE > 1\n",
    "    recipe.optim.config.lr = LR\n",
    "    recipe.optim.lr_scheduler.warmup_steps = WARMUP_STEPS\n",
    "    recipe.optim.lr_scheduler.min_lr = MIN_LR\n",
    "\n",
    "    return recipe\n",
    "\n",
    "\n",
    "recipe = configure_recipe()\n",
    "print(recipe)\n",
    "env_vars = {\n",
    "    \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",  # Disable caching NCCL communication buffer memory\n",
    "    \"NCCL_NVLS_ENABLE\": \"0\",  # Disable NVLink SHARP to save memory\n",
    "}\n",
    "executor = run.LocalExecutor(ntasks_per_node=recipe.trainer.devices, launcher=\"torchrun\", env_vars=env_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.run(recipe, executor=executor, name=EXP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH To be CHANGED\n",
    "!mv \"{ROOT_DIR}/Llama-3.1-8B-nemo-ft/checkpoints/model_name=0--val_loss=2.03-step=29-consumed_samples=3840.0\" \"{ROOT_DIR}/Llama-3.1-8B-nemo-ft/checkpoints/best\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
